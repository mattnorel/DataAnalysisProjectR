---
title: "Analysis of Herrings Dataset"
output: 
  md_document:
    variant: markdown_github
  html_document:
    toc: true
    toc_float: true
    keep_md: true
author: "Piotr Kaszuba & Mateusz Norel"
date: "`r format(Sys.time(), '%d %B %Y')`"
params:
  dataset_name: "sledzie.csv"
  dataset_url: "http://www.cs.put.poznan.pl/alabijak/emd/projekt/sledzie.csv"
  na_chars: "?"
---

```{r markdown_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Used Packages

Loading libaries:
```{r loading_libraries, results='hide', message=FALSE}
library("RCurl")
library(dplyr)
library(readr)
library(tidyr)
library(stringr)
library(knitr)
library(kableExtra)
library(mice)
library(VIM)
library(e1071)
library(ggplot2)
library(corrplot)
library(plotly)
library(caret)
library(doMC)
library(xgboost)
```
Listing of libraries loaded during code execution can be found in the cell below:
```{r printing_libraries, echo=FALSE}
s <- sessionInfo()
lib_str <- c()

for(p in s$loadedOnly){
  lib_str <- lib_str %>% append(p$Package)
}
str_c(lib_str, collapse=', ')
```
## Loading Data

Data is loded accoring to the metadata provided in the `params` object, where a location of a dataset (both local and url) is provided. Mentioned object contains also information about expected form of missing data. In this case 
```{r loading_data}
if(file.exists(params$dataset_name)) {
  dataset <- read.csv(params$dataset_name, na.strings = params$na_chars)
} else if(url.exists(params$dataset_url)){
  download.file(params$dataset_url, params$dataset_name)
  dataset <- read.csv(params$dataset_name, na.strings = params$na_chars)
} else {
  stop("There is no file nor url resource to work with!")
}
```

## Dataset

To see dimensionality we've used `dim` function:
```{r}
dim(dataset)
```
There are 52582 row and 16 columns.

Here are names of all columns available in a raw dataset:
```{r}
dataset %>%
  colnames %>%
  print
```

First sight at some rows from dataset will help us get better view on the data.
```{r}
head(dataset)
```


Names of some columns may be ambiguous. This is brief explanation:
```{r echo=FALSE}
lines <- read_lines("attributes.txt")
cat(paste(lines, collapse = "\n"))
```

After that dataset can be transferred to dyplr's `DataFrame` object.
By selecting just columns with `numeric` type and then checking if all column names match those from the raw set, we can make sure that all columns are ready for further work.
```{r}
herrings <- tbl_df(dataset)
herrings %>% 
  select_if(is.numeric) %>%
  colnames
```

It is also required to transform `xmonth` column to cathegorical, as its values are numbers of months.
```{r}
herrings <- herrings %>%
  mutate(xmonth = as.factor(xmonth))
```

## Missing Values
All columns were extracted correctly and do not contain anything different from numeric variables. But there are still missing values (`NA`s) in the dataframe. Before deciding what to do with them it is crucial to know how many rows have `NA` value.

We started with showing the list of columns that have at least one `NA` value.
```{r}
cols_with_na = c()
for(col in colnames(herrings)){
  if(any(is.na(herrings[col]))){
    cols_with_na <- cols_with_na %>% append(col)
  }
}
cols_with_na
```
There are `r length(cols_with_na)` such columns in herrings dataset. 

Before making any decision about missing data it is important to somehow visualize it. It is recommended to not reconstruct variable if more than 5% of the data is not available. Also it is worth knowing missing data pattern. If it's random it is good idea to try some imputation methods. But if it seems that there is some reason for data being not available it's better to consider other methods (profound analysis of data gathering process is a way to start).

Following plots contains interesting information about missing data. It was done using VIM's `aggr` function on dataset with just those columns which contain at least `NA` value. Left figure, "Histogram of missing data" informs us how many values of each column are missing. We can see that it is around 3% of all values for each column. It is less than mentioned 5% so data reconstruction is worth considering. More than that, as all columns has around 3% missing data it is first hint that not availability might be caused by some random process. The right chart, "Missing data pattern", shows what is the pattern of missing data. All combinations of missing data were captured from the dataset and presented. Red color symbolizes `NA`, while blue indicate that correct value is available. Most common combinations are at the very bottom. Moving to the top there are less and less frequent combinations. 

It seems that there are no strong relations between missing values. Next step is an imputation.
```{r message=FALSE}
just_with_na <- herrings %>% select(cols_with_na)
aggr_plot <- aggr(just_with_na, col=c('navyblue','red'), numbers=FALSE, bars=TRUE, sortVars=TRUE, labels=names(just_with_na), cex.axis=.8, gap=3, ylab=c("Histogram of missing data","Missing data pattern"))
```

## Imputing Missing Data

In order to reconstruct missing data we used a `mice` library (Multivariate Imputation By Chained Equations). Seed for a random generator was set to 17. MICE tries to imput values in places of missing data based on the context from other attributes. One of attributes, `sal`, had to be excluded from this step as its participation resulted in error. MICE supports many methods from which one could choose by first trying its reconstruction results on smaller subsequence of data and compare that to ground truth. We omitted that step and went straight to fairly good `pmm` imputation method. Its name stands for "Predictive Mean Matching.
```{r message=FALSE}
set.seed(17)
imputed <- mice(herrings,predictorMatrix = quickpred(herrings, exclude = c('sal')), meth='pmm', maxit=1)
tmp_data <- complete(imputed,1)
imputed_data = mutate(tmp_data, sal = herrings$sal)
```

## Attribute-wise Analysis

We started studying attributes by showing their summary which contains basic statistical metrics. There are no missing values as we got rid of them earlier.
```{r}
kable_styling(kable(summary(imputed_data)))
```

Figure below presents correlation matrix (or rather its upper part). Attribute `length` is negatively correlated with sea surface temperature, but besides that it is fairly weakly connected with other attributes.
```{r}
#corrplot(imputed_data, type="upper", order="hclust", tl.col="black", tl.srt=45)
cor_data <- select(imputed_data, -c('xmonth', 'X'))
corrplot(cor(cor_data), type='upper', order='hclust', tl.col='black', tl.srt=55, title="Correlation matrix")
```

The chart below presents how a leanth of herrings was changing over time. Blue line is an effect of smooth function. It allows us to clearly see trend present in data. There is noticeable breakpoint.
```{r message=FALSE}
tmp <- imputed_data %>%
          dplyr::slice(seq(1, nrow(imputed_data), 50))

p <- ggplot(tmp, aes(x=X, y=length)) +
    geom_point(shape=1) + stat_smooth()
p <- ggplotly(p)
p
```

It is standard practice to check distributions of data plotting histograms and measure the distribution skewness.
```{r}

for(col in colnames(imputed_data)){
  if(col != "X"){
  plot <- qplot(imputed_data[[col]], xlab=col) + ggtitle("")
  print(plot)
  print(skewness(imputed_data[[col]]))
  }
}
```
There are some variables that should be looked closer upon. For those that skewness was > 2 a log transform will be used. There could be more analysis done to all of the variables but authors decide that it is enough to further investigate only some of them. Depending on the number of zeros there will be a constant (before a log transform) added or dummy variable introduced that takes value of 1 if the column was zero and 0 otherwise. This way a more suitable distribution can be obtained and important information regarding zeros in the data kept.

```{r} 
number_of_zeros = NROW(imputed_data$cfin1[imputed_data$cfin1== 0])

```

For the variable cfin1 - log transform was used and a dummy variable introduced. There were  `r number_of_zeros/NROW(imputed_data)`% zero values in this column.

```{r message=FALSE}
  print(paste("Number of zeros: ", number_of_zeros))
  imputed_data$cfin1Changed[imputed_data$cfin1 > 0] <- log(imputed_data$cfin1[imputed_data$cfin1 > 0])
  imputed_data$cfin1Changed[imputed_data$cfin1 <= 0] <- 0
  plot <- qplot(imputed_data$cfin1Changed[imputed_data$cfin1 > 0], xlab="cfin1", binwidth=0.05)
  print(plot)
  
  imputed_data$cfin1Zero <- as.numeric(imputed_data$cfin1 == 0)
  print(qplot(imputed_data$cfin1Zero,xlab="cfin1Zero"))
```
```{r} 
number_of_zeros = NROW(imputed_data$cfin2[imputed_data$cfin2== 0])

```

Same technique was used for cfin2 variable as there were still `r number_of_zeros/NROW(imputed_data)`% records with zero values.
```{r message=FALSE}
  print(paste("Number of zeros: ", number_of_zeros))
  imputed_data$cfin2Changed[imputed_data$cfin2 > 0] <- log(imputed_data$cfin2[imputed_data$cfin2 > 0])
  imputed_data$cfin2Changed[imputed_data$cfin2 <= 0] <- 0
  plot <- qplot(imputed_data$cfin2Changed[imputed_data$cfin2 > 0], xlab="cfin2", binwidth=0.05)
  print(plot)
  
  imputed_data$cfin2Zero <- as.numeric(imputed_data$cfin2 == 0)
  print(qplot(imputed_data$cfin2Zero,xlab="cfin2Zero"))
  
```
```{r} 
number_of_zeros = NROW(imputed_data$chel1[imputed_data$chel1== 0])

```

For 'chel1' there were `r number_of_zeros/NROW(imputed_data)`% zero values (<5%) that is why log transform with an added constant was preferred. The 5% value is not a rule but rather a threshold authors decided on. More appropriate solution could be to test all possible variable setups.
```{r}
  print(paste("Number of zeros: ", number_of_zeros))
  imputed_data$chel1Changed <- log(imputed_data$chel1 +1)
  
  plot <- qplot(imputed_data$chel1Changed, xlab="chel1", binwidth=0.05)
  print(plot)
  
```
```{r} 
number_of_zeros = NROW(imputed_data$lcop1[imputed_data$lcop1== 0])

```

"lcop1" was only log transformed with an added constant. There were `r number_of_zeros/NROW(imputed_data)`% zero values!

```{r}
  print(paste("Number of zeros: ", number_of_zeros))
  imputed_data$lcop1Changed <- log(imputed_data$lcop1 +1)
  plot <- qplot(imputed_data$lcop1Changed, xlab="lcop1", binwidth=0.05)
  print(plot)
  
```

## Regression

First we start with turning `xmonth` into dummy variable. Now there are new columns for each month. Zero indicates that sample was not taken in this month and one if it was taken in month which is specified in the name of the column.
```{r}
reg_data <- model.matrix(X ~ ., imputed_data)
head(reg_data)
```

Some attributes in this dataset has values measured in tens of thousands while others never exceeds a value of one. If our data will stay in that form some attributes will contribute greatly to the final result while others will be ignored. To make sure that all data equaly participates in learning process it has to be normalized in one way or another. We decided to use simple scale and centering methods from `caret` library. But before that data had to be divided into test and training set. Fitting normalization model can be done just on training dataset, as it would introduce data leak if conducted on the whole dataset.

```{r}
ctrl <- trainControl(method = "cv", number=8, preProcOptions = c(pcaComp=32), allowParallel = TRUE)
grid <- expand.grid()
lm <- train(length~., data = reg_data, method='lm', trControl=ctrl, preProcess=c('center', 'scale', 'pca'))
lm
```

```{r}
registerDoMC(cores=4)
ctrl <- trainControl(method = "cv", number=8, preProcOptions = c(psaComp=3), allowParallel = TRUE)
grid <- expand.grid(max_depth=c(3,4,5), 
                    subsample=c(1.0), 
                    nrounds=c(200),
                    eta=c(0.4, 0.3),
                    gamma=c(0.001,0.01),
                    colsample_bytree=c(0.7),
                    min_child_weight=c(1.0, 0.5))
xgb_tree <- train(length~., data = reg_data, method='xgbTree', trControl=ctrl, tuneGrid=grid, preProcess=c('center', 'scale'))
xgb_tree
```